{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1enO4AT5yUKw",
        "outputId": "b0783c56-e8ef-421f-fe01-c4b58d9c8260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.5\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8SNK2whNhP0",
        "outputId": "9c7c891d-f231-4da5-c3d6-fa8760f5f093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unzipped to: /content/aresumes_folder\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/aresumes.zip'\n",
        "extract_to = '/content/aresumes_folder'\n",
        "\n",
        "# Create target directory if it doesn't exist\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(f\"Unzipped to: {extract_to}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziTrhNq4yAL_",
        "outputId": "72b7c8fe-6545-46c6-929f-2f9d5cb0789f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'education': 'Bachelor of Science in Digital Transformation',\n",
            " 'email': 'idpzoxmg@email.com',\n",
            " 'experience_years': 14,\n",
            " 'links': 'https://linkedin.com/in/wuowchpyrr; https://github.com/qdhtteejif',\n",
            " 'name': 'Charlie Green',\n",
            " 'phone': '+1 9841380772\\n',\n",
            " 'role': 'Statistical Programmer',\n",
            " 'skills': 'sql; tableau; power bi; r; postgresql; c++; machine learning; '\n",
            "           'statistics; hadoop; git; docker; model evaluation; model '\n",
            "           'deployment; hugging face',\n",
            " 'summary': 'A dedicated and results-driven Statistical Programmer with over '\n",
            "            '14 years of experience in data\\n'\n",
            "            'analysis, machine learning, and predictive modeling.\\n'\n",
            "            'Skilled in transforming business needs into\\n'\n",
            "            'technical solutions using modern data science tools and '\n",
            "            'practices. Passionate about solving real-\\n'\n",
            "            'world problems through data-driven approaches and delivering '\n",
            "            'measurable outcomes.'}\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with fitz.open(file_path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "def extract_email(text):\n",
        "    match = re.search(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
        "    return match.group() if match else \"\"\n",
        "\n",
        "def extract_phone(text):\n",
        "    match = re.search(r'\\+?\\d[\\d\\s\\-]{8,15}', text)\n",
        "    return match.group() if match else \"\"\n",
        "\n",
        "def extract_links(text):\n",
        "    links = []\n",
        "\n",
        "    # Match LinkedIn\n",
        "    linkedin_match = re.search(r'(linkedin\\.com/in/[a-zA-Z0-9_-]+)', text, re.IGNORECASE)\n",
        "    if linkedin_match:\n",
        "        links.append(\"https://\" + linkedin_match.group(1))\n",
        "\n",
        "    # Match GitHub\n",
        "    github_match = re.search(r'(github\\.com/[a-zA-Z0-9_-]+)', text, re.IGNORECASE)\n",
        "    if github_match:\n",
        "        links.append(\"https://\" + github_match.group(1))\n",
        "\n",
        "    return links\n",
        "\n",
        "def extract_name(text):\n",
        "    lines = text.strip().split('\\n')[:10]\n",
        "    for line in lines:\n",
        "        clean = line.strip()\n",
        "        # Ignore if line contains contact info or known words\n",
        "        if \"@\" in clean or any(x in clean.lower() for x in [\"linkedin\", \"github\", \"email\", \"phone\", \"curriculum\", \"vitae\"]):\n",
        "            continue\n",
        "        words = clean.split()\n",
        "        if 1 < len(words) <= 4 and all(w[0].isupper() for w in words if w):\n",
        "            return clean\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_skills(text):\n",
        "    skill_keywords = [\n",
        "    \"Python\", \"SQL\", \"Tableau\", \"Power BI\", \"R\", \"Pandas\", \"NumPy\", \"Scikit-learn\",\n",
        "    \"MySQL\", \"PostgreSQL\", \"Excel\", \"Java\", \"C++\", \"Machine Learning\", \"Deep Learning\",\n",
        "    \"Statistics\", \"Data Visualization\", \"Data Mining\", \"Big Data\", \"Hadoop\", \"Spark\",\n",
        "    \"Airflow\", \"Git\", \"Docker\", \"Kubernetes\", \"Data Wrangling\", \"Data Cleaning\",\n",
        "    \"Natural Language Processing\", \"Computer Vision\", \"Seaborn\", \"Matplotlib\", \"Plotly\",\n",
        "    \"ETL Pipelines\", \"AWS\", \"Azure\", \"Google Cloud Platform (GCP)\", \"Feature Engineering\",\n",
        "    \"Model Evaluation\", \"Model Deployment\", \"MLOps\", \"Data Lakes\", \"Snowflake\",\n",
        "    \"Data Governance\", \"A/B Testing\", \"Data Ethics\", \"Bayesian Inference\", \"EDA\",\n",
        "    \"Transformers\", \"Hugging Face\", \"FastAPI\", \"Dask\"\n",
        "    ]\n",
        "    skill_keywords = [skill.lower() for skill in skill_keywords]\n",
        "    text = text.lower()\n",
        "    return [skill for skill in skill_keywords if skill in text]\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_degrees(raw_text):\n",
        "    \"\"\"\n",
        "    Extracts academic degree information from raw CV/resume text.\n",
        "\n",
        "    Args:\n",
        "        raw_text (str): The raw text from a CV or resume.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of extracted degree strings.\n",
        "    \"\"\"\n",
        "    degree_pattern = re.compile(\n",
        "        r'\\b(Bachelor|Master|Doctor|Associate)[\\w\\s]* in [^\\n]+',\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    match = degree_pattern.search(raw_text)\n",
        "    return match.group().strip() if match else ''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_experience_years(text):\n",
        "    match = re.findall(r'(\\d+)\\+?\\s*years?', text.lower())\n",
        "    return max([int(m) for m in match], default=0)\n",
        "\n",
        "def extract_role(text):\n",
        "    role_keywords = [\n",
        "    \"Data Analyst\", \"Data Scientist\", \"Business Analyst\", \"Machine Learning Engineer\",\n",
        "    \"AI Engineer\", \"BI Developer\", \"Quantitative Analyst\", \"Data Engineer\",\n",
        "    \"Big Data Engineer\", \"Research Scientist\", \"Analytics Consultant\",\n",
        "    \"Decision Scientist\", \"Data Architect\", \"Data Strategist\", \"Statistical Programmer\",\n",
        "    \"ML Ops Engineer\", \"NLP Engineer\", \"Computer Vision Engineer\", \"Product Data Analyst\",\n",
        "    \"Data Journalist\", \"Business Intelligence Analyst\", \"Healthcare Data Analyst\",\n",
        "    \"Customer Insights Analyst\", \"Operations Analyst\", \"Marketing Data Analyst\",\n",
        "    \"Financial Data Analyst\", \"Growth Analyst\", \"Risk Analyst\", \"Fraud Analyst\",\n",
        "    \"Research Analyst\", \"CRM Analyst\", \"Data Quality Analyst\", \"IoT Data Analyst\",\n",
        "    \"Sports Data Scientist\", \"Retail Data Analyst\", \"Actuarial Data Scientist\",\n",
        "    \"Data Science Manager\", \"AI Product Manager\", \"Data Analytics Lead\",\n",
        "    \"Public Policy Data Analyst\", \"Transportation Data Analyst\", \"HR Data Analyst\",\n",
        "    \"Climate Data Scientist\", \"Geospatial Data Analyst\", \"Open Source Contributor (Data)\",\n",
        "    \"Data Science Instructor\", \"AI Ethicist\", \"Knowledge Graph Engineer\",\n",
        "    \"Data Consultant\", \"Personalized Systems Specialist\"\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    role_keywords = [role.lower() for role in role_keywords]\n",
        "\n",
        "    for role in role_keywords:\n",
        "        if role in text_lower:\n",
        "            return role.title()\n",
        "\n",
        "    return \"Unknown\"\n",
        "def extract_summary(text):\n",
        "    # Use case-insensitive regex to find 'Summary' and 'Skills'\n",
        "    pattern = re.compile(r'Summary(.*?)Core Skills', re.DOTALL | re.IGNORECASE)\n",
        "    match = pattern.search(text)\n",
        "    if match:\n",
        "        summary = match.group(1).strip()\n",
        "        return summary\n",
        "    return \"\"\n",
        "def parse_resume(file_path):\n",
        "    text = extract_text_from_pdf(file_path)\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"summary\": extract_summary(text),\n",
        "        \"name\": extract_name(text),\n",
        "        \"email\": extract_email(text),\n",
        "        \"phone\": extract_phone(text),\n",
        "        \"links\": \"; \".join(extract_links(text)),\n",
        "        \"role\": extract_role(text),\n",
        "        \"skills\": \"; \".join(extract_skills(text)),\n",
        "        \"education\": extract_degrees(text),\n",
        "        \"experience_years\": extract_experience_years(text)\n",
        "    }\n",
        "\n",
        "# --- Test on your resume\n",
        "if __name__ == \"__main__\":\n",
        "    parsed = parse_resume(\"/content/aresumes_folder/cv_1.pdf\")  # update if filename changes\n",
        "    from pprint import pprint\n",
        "    pprint(parsed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SZCGrNJl46Sq"
      },
      "outputs": [],
      "source": [
        "#read 1000 files\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "def write_to_csv(data, csv_filename):\n",
        "    fieldnames = [\"name\", \"email\", \"phone\", \"links\", \"role\", \"skills\", \"education\", \"experience_years\", \"summary\"]\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for entry in data:\n",
        "            writer.writerow(entry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zn-XLDsb6xrg"
      },
      "outputs": [],
      "source": [
        "def process_resumes_in_folder(folder_path, output_csv=\"parsed_resumes.csv\"):\n",
        "    parsed_data = []\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    for pdf_file in tqdm(pdf_files[:1000], desc=\"Parsing resumes\"):\n",
        "        file_path = os.path.join(folder_path, pdf_file)\n",
        "        try:\n",
        "            data = parse_resume(file_path)\n",
        "            parsed_data.append(data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_file}: {e}\")\n",
        "\n",
        "    write_to_csv(parsed_data, output_csv)\n",
        "    print(f\"\\nDone. Parsed {len(parsed_data)} files and saved to {output_csv}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7hZtRhX62sv",
        "outputId": "de469e69-1069-4c09-cedc-cc098081dda2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parsing resumes: 100%|██████████| 1000/1000 [00:12<00:00, 81.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Done. Parsed 1000 files and saved to parsed_resumes.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content/aresumes_folder\"  # change to your folder containing PDFs\n",
        "    process_resumes_in_folder(folder_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
